import pandas as pd
import torch
from transformers import BertTokenizer, BertModel
import numpy as np
from sklearn.ensemble import IsolationForest
from sklearn.cluster import DBSCAN
from datetime import datetime

# -------------------------
# Layer 1: Data & Discovery
# -------------------------

def feature_engineering(logs: pd.DataFrame) -> pd.DataFrame:
    """
    Turn raw logs into feature vectors per time window (example: 5 min windows).
    Each row = one window with engineered features.
    """
    logs['timestamp'] = pd.to_datetime(logs['timestamp'])
    logs.set_index('timestamp', inplace=True)

    # Resample into 5 min windows
    grouped = logs.resample("5T")

    features = pd.DataFrame({
        "event_count": grouped.size(),
        "unique_users": grouped['user'].nunique(),
        "unique_ips": grouped['ip'].nunique(),
        "burstiness": grouped.size() / (grouped.size().mean() + 1),
    })
    features = features.fillna(0)
    return features

def anomaly_detection(features: pd.DataFrame) -> pd.Series:
    """
    Isolation Forest to flag anomalous windows.
    """
    model = IsolationForest(contamination=0.05, random_state=42)
    preds = model.fit_predict(features)
    return pd.Series(preds, index=features.index)

# -------------------------------
# Layer 2: Validation & Benchmark
# -------------------------------

def validate_metrics(features: pd.DataFrame, anomalies: pd.Series):
    """
    Simple validation: compare anomaly flags with standard metrics.
    """
    metrics = {
        "avg_event_rate": features["event_count"].mean(),
        "anomaly_rate": (anomalies == -1).mean(),
    }
    return metrics

# ------------------------------------------
# Layer 3: Log â†” Non-Log Metrics Integration
# ------------------------------------------

def pair_metrics(log_metrics: dict, nonlog_metrics: dict):
    """
    Combine incident-centric (logs) with posture-centric (non-logs).
    """
    return {**log_metrics, **nonlog_metrics}

# -------------------------------
# Layer 4: Business Correlation
# -------------------------------

def correlate_with_business(metrics: dict, business_outcomes: dict):
    """
    Very simple correlation placeholder.
    In real use: regression/statistics to see if security metrics explain business KPIs.
    """
    correlations = {}
    for k, v in metrics.items():
        for b, bv in business_outcomes.items():
            correlations[(k, b)] = round(np.corrcoef([v], [bv])[0, 1], 2) if v != 0 else 0
    return correlations

# -------------------------------
# Example Usage
# -------------------------------
if __name__ == "__main__":
    # Sample synthetic log data
    logs = pd.DataFrame({
        "timestamp": pd.date_range("2025-09-01", periods=500, freq="T"),
        "user": np.random.choice(["alice", "bob", "charlie"], size=500),
        "ip": np.random.choice(["10.0.0.1", "10.0.0.2", "192.168.1.5"], size=500),
    })

    # Layer 1
    features = feature_engineering(logs)
    anomalies = anomaly_detection(features)

    # Layer 2
    log_metrics = validate_metrics(features, anomalies)

    # Layer 3 (pairing with a dummy non-log metric like patch compliance)
    nonlog_metrics = {"patch_compliance": 0.85, "MFA_coverage": 0.9}
    combined_metrics = pair_metrics(log_metrics, nonlog_metrics)

    # Layer 4 (business outcomes: uptime %, revenue impact etc.)
    business_outcomes = {"uptime": 0.995, "quarterly_revenue_growth": 0.04}
    correlations = correlate_with_business(combined_metrics, business_outcomes)

    print("Discovered Features:\n", features.head())
    print("\nLog Metrics:", log_metrics)
    print("\nCombined Metrics:", combined_metrics)
    print("\nCorrelation with Business KPIs:", correlations)
