from confluent_kafka import Consumer
from transformers import BertTokenizer, BertModel
import torch
from river import anomaly
from sklearn.ensemble import IsolationForest
import numpy as np
import joblib
import requests

# Kafka Consumer
conf = {
    'bootstrap.servers': "kafka-broker1:9092",
    'group.id': "log-consumer-group",
    'auto.offset.reset': 'earliest'
}
consumer = Consumer(conf)
consumer.subscribe(['logs-topic'])

# BERT
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')

def generate_embedding(log_line):
    inputs = tokenizer(log_line, return_tensors="pt", truncation=True, max_length=128)
    with torch.no_grad():
        outputs = model(**inputs)
    return outputs.last_hidden_state[:, 0, :].squeeze().numpy()

# River Streaming Model
river_model = anomaly.HalfSpaceTrees(seed=42, n_trees=25, height=15, window_size=250)

# Sklearn Batch Model
batch_model = IsolationForest(n_estimators=100, contamination=0.01)
embeddings_batch = []

# Slack Alert
def send_slack_alert(log_line, score):
    webhook_url = "https://hooks.slack.com/services/XXXX/YYYY/ZZZZ"
    payload = {"text": f"ðŸš¨ Anomaly detected!\nScore: {score:.2f}\nLog: {log_line}"}
    requests.post(webhook_url, json=payload)

# Streaming Learning
def stream_learn(embedding):
    emb_dict = {str(i): float(embedding[i]) for i in range(len(embedding))}
    score = river_model.score_one(emb_dict)
    river_model.learn_one(emb_dict)
    return score

# Batch Training
def batch_train(X):
    global batch_model
    batch_model.fit(X)
    joblib.dump(batch_model, "batch_model.pkl")

def batch_predict(embedding):
    return batch_model.decision_function([embedding])[0]

# Parameters
ANOMALY_THRESHOLD = 0.7
LOG_COUNT = 0
RETRAIN_WINDOW = 10000

# Main Loop
while True:
    msg = consumer.poll(timeout=1.0)
    if msg is None:
        continue
    if msg.error():
        continue

    log_line = msg.value().decode('utf-8')
    embedding = generate_embedding(log_line)

    # Streaming score
    stream_score = stream_learn(embedding)

    # Batch score (if batch model trained)
    if embeddings_batch:
        batch_score = batch_predict(embedding)
    else:
        batch_score = 0.0

    # Combine scores
    final_score = (stream_score + batch_score) / 2

    if final_score > ANOMALY_THRESHOLD:
        send_slack_alert(log_line, final_score)
    else:
        with open("normal_logs.txt", "a") as f:
            f.write(log_line + "\n")

    embeddings_batch.append(embedding)
    LOG_COUNT += 1

    if LOG_COUNT % RETRAIN_WINDOW == 0 and embeddings_batch:
        print("Retraining batch model...")
        batch_train(np.array(embeddings_batch))
        embeddings_batch = []
